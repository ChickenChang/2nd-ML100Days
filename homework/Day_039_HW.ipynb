{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "清楚了解 L1, L2 的意義與差異為何，並了解 LASSO 與 Ridge 之間的差異與使用情境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請閱讀相關文獻，並回答下列問題\n",
    "\n",
    "[脊回歸 (Ridge Regression)](https://blog.csdn.net/daunxx/article/details/51578787)\n",
    "[Linear, Ridge, Lasso Regression 本質區別](https://www.zhihu.com/question/38121173)\n",
    "\n",
    "1. LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "#### Robert Tibshirani最初使用Lasso來提高預測的準確性與回歸模型的可解釋性，他修改了模型擬合的過程，在協變量中只選擇一個子集應用到最終模型中，而非用上全部協變量。這是基於有著相似目的，但方法有所不同的Breiman的非負參數推斷。在Lasso之前，選擇模型中協變量最常用的方法是移步選擇，這種方法在某些情況下是準確的，例如一些協變量與模型輸出值有強相關性情況。然而在另一些情況下，這種方法會讓預測結果更差。在當時，嶺回歸是提高模型預測準確性最常用的方法。嶺回歸可以通過縮小大的回歸係數來減少過擬合從而改善模型預測偏差。但是它並不選擇協變量，所以對模型的準確構建和解釋沒有幫助。Lasso結合了上述的兩種方法，它通過強制讓回歸係數絕對值之和小於某固定值，即強制一些回歸係數變為0，有效地選擇了不包括這些回歸係數對應的協變量的更簡單的模型。這種方法和嶺回歸類似，在嶺回歸中，回歸係數平方和被強制小於某定值，不同點在於嶺回歸只改變係數的值，而不把任何值設為0。\n",
    "2. 當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n",
    "#### 可以。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
